{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-on Modul 3: Pipeline Inference Dasar (PyTorch & Hugging Face)\n",
        "_Repository GitHub: https://github.com/Komdigi-x-BINAR-LLM/Module-03_\n",
        "\n",
        "**Tujuan:** Mempraktikkan alur kerja dasar dari teks mentah hingga mendapatkan output internal model (`last_hidden_state`)."
      ],
      "metadata": {
        "id": "M9RyGHpTLEdh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RT1gvWaDqAeD"
      },
      "outputs": [],
      "source": [
        "# Setup Awal (Jalankan sel ini jika di Colab atau environment baru)\n",
        "# !pip install transformers torch notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "from transformers import AutoTokenizer, AutoModel # Gunakan AutoModel dasar\n",
        "import torch\n",
        "import logging\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# --- Force re-configuration logging for notebooks ---\n",
        "importlib.reload(logging)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, # Tampilkan pesan INFO\n",
        "    format='%(levelname)s: %(message)s',\n",
        "    stream=sys.stdout # Paksa output ke stdout (yang ditangkap notebook)\n",
        ")\n",
        "# -----------------------------\n",
        "\n",
        "logging.info(\"Setup selesai. Library siap digunakan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utv-84zNKjfM",
        "outputId": "5780df07-488a-4f65-bd55-260736be409f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Setup selesai. Library siap digunakan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Langkah 1: Load Model & Tokenizer (Konsep 3.2)\n",
        "Kita akan memuat model Encoder-only yang ringan (DistilBERT) dan tokenizer-nya."
      ],
      "metadata": {
        "id": "StpdqPJfLaW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\" # Model Encoder-only ringan\n",
        "logging.info(f\"Mencoba memuat tokenizer untuk '{model_name}'...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    logging.info(\"Tokenizer berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Gagal memuat tokenizer: {e}\")\n",
        "    raise\n",
        "\n",
        "logging.info(f\"Mencoba memuat model '{model_name}'...\")\n",
        "try:\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    logging.info(\"Model berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Gagal memuat model: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SEeph90KoOM",
        "outputId": "f628f580-52b1-4059-cd7a-91e260a52dc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Mencoba memuat tokenizer untuk 'distilbert-base-uncased'...\n",
            "INFO: Tokenizer berhasil dimuat.\n",
            "INFO: Mencoba memuat model 'distilbert-base-uncased'...\n",
            "INFO: loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
            "INFO: Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "INFO: Model berhasil dimuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Langkah 2: Siapkan Device (Konsep 3.1)\n",
        "Kita cek ketersediaan GPU dan pindahkan model ke device yang sesuai."
      ],
      "metadata": {
        "id": "XlB90klVLe1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    logging.info(\"GPU (cuda) terdeteksi. Model akan dipindahkan ke GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    logging.info(\"GPU tidak terdeteksi. Model akan menggunakan CPU.\")\n",
        "\n",
        "# Pindahkan model\n",
        "model.to(device)\n",
        "logging.info(f\"Model sekarang berada di device: {model.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3NpfZgvK279",
        "outputId": "b659db4e-2a7f-4935-fc82-f6787a9b5588"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: GPU tidak terdeteksi. Model akan menggunakan CPU.\n",
            "INFO: Model sekarang berada di device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Langkah 3: Siapkan Input & Preprocessing (Konsep 3.3)\n",
        "Kita akan memproses beberapa kalimat dengan panjang berbeda menggunakan tokenizer,\n",
        "menerapkan padding dan truncation, serta menghasilkan PyTorch tensor."
      ],
      "metadata": {
        "id": "eroelAvXLjEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kalimat_batch = [\n",
        "    \"Hugging Face itu keren!\", # Kalimat pendek\n",
        "    \"Ini adalah kalimat kedua yang sedikit lebih panjang.\", # Kalimat lebih panjang\n",
        "    \"Contoh kalimat ketiga ini dibuat sangat panjang sekali agar melebihi batas umum beberapa model dasar dan kita bisa melihat efek truncation jika diaktifkan nanti.\" # Kalimat sangat panjang\n",
        "]\n",
        "print(f\"Input Teks (Batch):\\n{kalimat_batch}\")\n",
        "\n",
        "# Tentukan max_length (misal, 20 untuk demonstrasi truncation)\n",
        "max_len = 20\n",
        "logging.info(f\"Melakukan tokenisasi dengan padding=True, truncation=True, max_length={max_len}, return_tensors='pt'...\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    kalimat_batch,\n",
        "    padding=True,       # Aktifkan dynamic padding\n",
        "    truncation=True,    # Aktifkan truncation\n",
        "    max_length=max_len, # Batasi panjang maksimal\n",
        "    return_tensors=\"pt\" # Minta output sebagai PyTorch Tensor\n",
        ")\n",
        "\n",
        "logging.info(\"Tokenisasi selesai.\")\n",
        "print(\"\\nHasil Tokenisasi (Dictionary Tensor):\")\n",
        "print(\"{\")\n",
        "for key, tensor in inputs.items():\n",
        "    print(f\"  '{key}':\")\n",
        "    print(f\"    Shape: {tensor.shape}\")\n",
        "    print(f\"    Tensor:\\n{tensor}\")\n",
        "print(\"}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzJejUOcK4Ij",
        "outputId": "b2a3fd70-b88f-4f42-e4f8-94df952e4701"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Teks (Batch):\n",
            "['Hugging Face itu keren!', 'Ini adalah kalimat kedua yang sedikit lebih panjang.', 'Contoh kalimat ketiga ini dibuat sangat panjang sekali agar melebihi batas umum beberapa model dasar dan kita bisa melihat efek truncation jika diaktifkan nanti.']\n",
            "INFO: Melakukan tokenisasi dengan padding=True, truncation=True, max_length=20, return_tensors='pt'...\n",
            "INFO: Tokenisasi selesai.\n",
            "\n",
            "Hasil Tokenisasi (Dictionary Tensor):\n",
            "{\n",
            "  'input_ids':\n",
            "    Shape: torch.Size([3, 20])\n",
            "    Tensor:\n",
            "tensor([[  101, 17662,  2227,  2009,  2226, 17710,  7389,   999,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1999,  2072, 15262, 14431, 19924, 18900, 16135,  6692,  8675,\n",
            "          7367,  4305, 23615,  3393,  5638,  2232,  6090,  8405,  2290,   102],\n",
            "        [  101,  9530,  3406,  2232, 19924, 18900, 17710,  3775,  3654,  1999,\n",
            "          2072,  4487,  8569,  4017,  6369,  4017,  6090,  8405,  2290,   102]])\n",
            "  'attention_mask':\n",
            "    Shape: torch.Size([3, 20])\n",
            "    Tensor:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observasi Hasil Tokenisasi:**\n",
        "* Perhatikan `shape` dari `input_ids` dan `attention_mask`. Angka terakhir (kolom) harusnya sama untuk semua kalimat dan tidak lebih dari `max_length`.\n",
        "* Lihat nilai `input_ids`. Apakah ada angka `0` (ID token [PAD]) di kalimat yang lebih pendek?\n",
        "* Lihat nilai `attention_mask`. Apakah ada angka `0` di posisi yang sama dengan token [PAD]?\n",
        "* Apakah kalimat yang sangat panjang terpotong?"
      ],
      "metadata": {
        "id": "5xXLxDCyLntc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Langkah 4: Jalankan Inference (Konsep 3.4)\n",
        "Kita pindahkan input tensor ke device yang sama dengan model dan jalankan inference."
      ],
      "metadata": {
        "id": "dyN6raBXLsvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pindahkan input tensor ke device\n",
        "logging.info(f\"Memindahkan input tensors ke {device}...\")\n",
        "try:\n",
        "    inputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n",
        "    logging.info(\"Input berhasil dipindahkan.\")\n",
        "\n",
        "    # Jalankan inference dalam context torch.no_grad()\n",
        "    logging.info(\"Menjalankan inference model...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs_on_device) # Unpack dictionary inputs\n",
        "    logging.info(\"Inference selesai.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Terjadi error saat memindahkan input atau inference: {e}\", exc_info=True)\n",
        "    outputs = None # Set outputs ke None jika gagal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDWvm9IK6XT",
        "outputId": "43cbb02c-db7f-4ede-994c-f0f2f186919c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Memindahkan input tensors ke cpu...\n",
            "INFO: Input berhasil dipindahkan.\n",
            "INFO: Menjalankan inference model...\n",
            "INFO: Inference selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Langkah 5: Analisis Output (Konsep 3.1 & 3.4)\n",
        "Kita periksa output utama dari `AutoModel` dasar, yaitu `last_hidden_state`."
      ],
      "metadata": {
        "id": "UFTn6VFrLvVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n### Analisis Output Model ###\")\n",
        "if outputs is not None and hasattr(outputs, 'last_hidden_state'):\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    logging.info(\"Mengakses 'last_hidden_state' dari output.\")\n",
        "\n",
        "    print(f\"\\nShape dari last_hidden_state: {last_hidden_states.shape}\")\n",
        "    print(f\"Tipe data (dtype): {last_hidden_states.dtype}\")\n",
        "    print(f\"Device: {last_hidden_states.device}\")\n",
        "\n",
        "    # Penjelasan Shape: [Batch Size, Sequence Length, Hidden Size]\n",
        "    # - Batch Size: Jumlah kalimat dalam input (harusnya 3).\n",
        "    # - Sequence Length: Panjang token setelah padding/truncation (harusnya sama dengan max_len=20).\n",
        "    # - Hidden Size: Dimensi vektor embedding internal model (768 untuk distilbert-base-uncased).\n",
        "    print(\"\\nPenjelasan Shape:\")\n",
        "    print(f\"- Dimensi 0 (Batch Size): {last_hidden_states.shape[0]} (Jumlah kalimat)\")\n",
        "    print(f\"- Dimensi 1 (Sequence Length): {last_hidden_states.shape[1]} (Panjang token)\")\n",
        "    print(f\"- Dimensi 2 (Hidden Size): {last_hidden_states.shape[2]} (Dimensi embedding model)\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nTidak dapat menganalisis output karena inference gagal atau output tidak sesuai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P26gVipTK9SD",
        "outputId": "84699874-2c2e-416c-a858-e73ba2edd60d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Analisis Output Model ###\n",
            "INFO: Mengakses 'last_hidden_state' dari output.\n",
            "\n",
            "Shape dari last_hidden_state: torch.Size([3, 20, 768])\n",
            "Tipe data (dtype): torch.float32\n",
            "Device: cpu\n",
            "\n",
            "Penjelasan Shape:\n",
            "- Dimensi 0 (Batch Size): 3 (Jumlah kalimat)\n",
            "- Dimensi 1 (Sequence Length): 20 (Panjang token)\n",
            "- Dimensi 2 (Hidden Size): 768 (Dimensi embedding model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eksperimen:**\n",
        "* Coba ganti `model_name` ke model lain (misal: \"bert-base-uncased\") di Langkah 1. Apakah `Hidden Size` berubah?\n",
        "* Ubah nilai `max_len` di Langkah 3. Apakah `Sequence Length` di output berubah?\n",
        "* Coba masukkan kalimat Anda sendiri di `kalimat_batch`."
      ],
      "metadata": {
        "id": "9ChEUK9qLxRV"
      }
    }
  ]
}